{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b36e7f-c492-4806-9181-044bc96b9a30",
   "metadata": {},
   "source": [
    "# Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e638019-69ba-4ea2-8ed2-50a47b5fe92a",
   "metadata": {},
   "source": [
    "With a prior EDA we preprocess our data and train our model.\n",
    "Linear models are easy to interpret, but they require more data preprocessing and rely on many hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eedec68b-2a0e-4100-963e-b2d8411ec4d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('train.csv') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc35cd5b-6e69-4b57-b890-51560410c49f",
   "metadata": {},
   "source": [
    "## 1.1 convert types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09afe1b3-f517-4d81-9e24-65d036ae0c9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['MSSubClass', 'MoSold', 'YearBuilt', 'YearRemodAdd', 'YrSold']] = df[['MSSubClass', 'MoSold', 'YearBuilt', 'YearRemodAdd', 'YrSold']].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5a648-d9bd-4cef-9ec0-cd318cfcc63e",
   "metadata": {},
   "source": [
    "## 1.2 drop variables with most values missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "598cb4d9-463a-47bb-a780-5c0b5347a12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.30  # 30%\n",
    "df = df.loc[:, df.isna().mean() <= threshold]\n",
    "\n",
    "threshold = 0.30  # 30%\n",
    "missing_ratio = df.isna().mean()\n",
    "\n",
    "# Columns to drop (more than 30% missing)\n",
    "dropped_columns_most_missing = missing_ratio[missing_ratio > threshold].index.tolist()\n",
    "\n",
    "# Drop from training data\n",
    "df = df.loc[:, missing_ratio <= threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32580f-1001-43d5-a6c8-5e1a55ff23c6",
   "metadata": {},
   "source": [
    "## 1.3 Fill missing values with \"Missing\" for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0074ae71-7d7b-4609-8bc2-d1c62804ed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berra\\AppData\\Local\\Temp\\ipykernel_9908\\1573740208.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[cat_cols] = df[cat_cols].fillna(\"Missing\")\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Fill missing values with \"Missing\"\n",
    "df[cat_cols] = df[cat_cols].fillna(\"Missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb0a7a-f25e-4632-887c-be0d0652d3ae",
   "metadata": {},
   "source": [
    "## 1.4 Fill missing values with the mean for quantitative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d97cffdc-ceb4-4bf2-9858-6dbcecf47ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save numeric means for missing test values\n",
    "means = df.select_dtypes(include='number').mean()\n",
    "\n",
    "#imputer numeric par moyenne dans train\n",
    "df = df.fillna(df.select_dtypes(include='number').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45595f-bcb4-471d-be8b-8cc23688b38e",
   "metadata": {},
   "source": [
    "## 1.5 create variables for clarity HouseAge, YearsSinceRemod..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ffe318cf-8074-46ea-aaf6-d9c4295b0a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dropped = df.copy()\n",
    "\n",
    "#'YearBuilt', 'YearRemodAdd, 'MoSold', 'YrSold' convert to quantitative\n",
    "# List of columns to convert\n",
    "columns_to_convert = ['YearBuilt', 'YearRemodAdd', 'MoSold', 'YrSold']\n",
    "\n",
    "# Convert each to integer safely\n",
    "for col in columns_to_convert:\n",
    "    df_dropped.loc[:, col] = df_dropped[col].astype(int)\n",
    "\n",
    "# Set reference year\n",
    "reference_year = df_dropped['YrSold'].max()\n",
    "\n",
    "# Create quantitative features safely\n",
    "df_dropped.loc[:, 'HouseAge'] = reference_year - df_dropped['YearBuilt']\n",
    "df_dropped.loc[:, 'YearsSinceRemod'] = reference_year -df_dropped['YearRemodAdd']\n",
    "df_dropped.loc[:, 'TimeIndex'] = (\n",
    "    (df_dropped['YrSold'] - df_dropped['YrSold'].min()) * 12 + df_dropped['MoSold']\n",
    ")\n",
    "\n",
    "df_dropped = df_dropped.drop([\n",
    "    'YearBuilt', 'YearRemodAdd', 'YrSold', 'MoSold'\n",
    "], axis=1) \n",
    "\n",
    "# Convert object columns to numeric, forcing errors to NaN\n",
    "cols_to_convert = ['HouseAge', 'YearsSinceRemod', 'TimeIndex']\n",
    "df_dropped[cols_to_convert] = df_dropped[cols_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ddd0e-165e-44f7-bcc4-d167ae2b9131",
   "metadata": {},
   "source": [
    "## 1.6 Correct skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2027cacc-d68f-43db-bc11-2860f68ab11b",
   "metadata": {},
   "source": [
    "For variables with a significant skewness from the test pandas.DataFrame.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e9de3b9-4341-44bb-875a-8ec3500b0e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 1: only binary no log\n",
    "\n",
    "#Most houses don't have low-quality finished square footage.\n",
    "#Very few do, with highly varied amounts.\n",
    "cols_to_binary_only_0 = ['BsmtHalfBath', 'EnclosedPorch', 'ScreenPorch']\n",
    "\n",
    "for col in cols_to_binary_only_0:\n",
    "    df_dropped[f'Has{col}'] = (df_dropped[col] > 0).astype(int)\n",
    "    df_dropped.drop(columns=[col], inplace=True)\n",
    "\n",
    "# = 1 or not\n",
    "\n",
    "df_dropped['HasKitchen'] = (df_dropped['KitchenAbvGr'] == 1).astype(int)\n",
    "df_dropped.drop(columns=['KitchenAbvGr'], inplace=True)\n",
    "\n",
    "\n",
    "#case 2: binary+log\n",
    "\n",
    "cols = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '2ndFlrSF', 'WoodDeckSF', 'OpenPorchSF']  # Replace with actual column names\n",
    "\n",
    "for col in cols:\n",
    "    df_dropped[f'Has{col}'] = (df_dropped[col] > 0).astype(int)\n",
    "    df_dropped[f'{col}_log'] = np.log1p(df_dropped[col])\n",
    "\n",
    "#case 3: just drop \n",
    "df_dropped.drop(columns=['LowQualFinSF','3SsnPorch', 'PoolArea', 'MiscVal'], inplace=True)\n",
    "\n",
    "# case 4: log only\n",
    "\n",
    "df_dropped.drop(columns=[ 'TimeIndex'], inplace=True)# cyclique, pas significative\n",
    "\n",
    "# List of variables to transform\n",
    "vars_to_log = ['LotFrontage', 'LotArea', 'TotalBsmtSF', 'GrLivArea',  'SalePrice', 'BsmtUnfSF', '1stFlrSF']\n",
    "\n",
    "# Create log-transformed versions with \"_log\" suffix\n",
    "for col in vars_to_log:\n",
    "    df_dropped[col + '_log'] = np.log1p(df_dropped[col])  # log1p handles zero safely\n",
    "\n",
    "# Drop the original columns\n",
    "df_dropped.drop(columns=vars_to_log, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cee469-811f-48b8-a036-db23b2c5fd3a",
   "metadata": {},
   "source": [
    "## 1.7 Cap outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5aa6b7a7-6c96-4873-9835-58aa546a261f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cap outliers\n",
    "#outliers cap\n",
    "\n",
    "num_features = [col for col in df_dropped.select_dtypes(include='number') if col != 'Id']\n",
    " \n",
    "# Store the limits for each column\n",
    "caps = {}\n",
    "\n",
    "for col in num_features:\n",
    "    q_low = df_dropped[col].quantile(0.01)\n",
    "    q_high = df_dropped[col].quantile(0.99)\n",
    "    \n",
    "    # Save the thresholds\n",
    "    caps[col] = (q_low, q_high)\n",
    "    \n",
    "    # Apply clipping\n",
    "    df_dropped[col] = df_dropped[col].clip(lower=q_low, upper=q_high)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a5a39-abdc-4f05-85ac-755e027d2517",
   "metadata": {},
   "source": [
    "## 1.8 Categorical variable (merge rare categories into the most frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "705933ca-0d59-4a52-8b59-e2f0ab083f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'MSZoning': 'Other' merged into most frequent category 'RL'\n",
      "'Street': 'Other' merged into most frequent category 'Pave'\n",
      "'Utilities': 'Other' merged into most frequent category 'AllPub'\n",
      "'LotConfig': 'Other' merged into most frequent category 'Inside'\n",
      "'LandSlope': 'Other' merged into most frequent category 'Gtl'\n",
      "'Condition2': 'Other' merged into most frequent category 'Norm'\n",
      "'RoofMatl': 'Other' merged into most frequent category 'CompShg'\n",
      "'ExterQual': 'Other' merged into most frequent category 'TA'\n",
      "'HeatingQC': 'Other' merged into most frequent category 'Ex'\n",
      "'GarageQual': 'Other' merged into most frequent category 'TA'\n"
     ]
    }
   ],
   "source": [
    "def clean_categorical_variables(\n",
    "    df, \n",
    "    target_col='SalePrice_log', \n",
    "    threshold=0.03, \n",
    "    tol=0.10, \n",
    "    min_count=30\n",
    "):\n",
    "    \"\"\"\n",
    "    Cleans all categorical variables:\n",
    "    - Fills missing values\n",
    "    - Groups rare non-predictive categories into 'Other'\n",
    "    - Merges 'Other' into closest price category if too small\n",
    "\n",
    "    Returns modified DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    global_mean = df[target_col].mean()\n",
    "    mappings = {}\n",
    "    for col in cat_cols:\n",
    "        # Step 1: Fill missing\n",
    "        df[col] = df[col].fillna(\"Missing\")\n",
    "\n",
    "        # Step 2: Frequency and stats\n",
    "        freq = df[col].value_counts(normalize=True)\n",
    "        rare_cats = freq[freq < threshold].index\n",
    "        stats = df.groupby(col)[target_col].agg(['count', 'mean'])\n",
    "\n",
    "        # Step 3: Decide which rare categories to keep\n",
    "        keep_rare = []\n",
    "        group_rare = []\n",
    "\n",
    "        for cat in rare_cats:\n",
    "            count = stats.loc[cat, 'count']\n",
    "            mean = stats.loc[cat, 'mean']\n",
    "            deviation = abs(mean - global_mean) / global_mean\n",
    "\n",
    "            if count >= min_count and deviation > tol:\n",
    "                keep_rare.append(cat)\n",
    "            else:\n",
    "                group_rare.append(cat)\n",
    "\n",
    "        # Step 4: Replace rare with 'Other'\n",
    "        df[col] = df[col].apply(lambda x: 'Other' if x in group_rare else x)\n",
    "\n",
    "        final_merge_target = None\n",
    "        # Step 5: Merge 'Other' if too small\n",
    "\n",
    "        if 'Other' in df[col].values:\n",
    "            other_mask = df[col] == 'Other'\n",
    "            if other_mask.sum() < min_count:\n",
    "                # Merge 'Other' into the most frequent existing category (excluding 'Other')\n",
    "                final_merge_target = df.loc[~other_mask, col].value_counts().idxmax()\n",
    "                df.loc[other_mask, col] = final_merge_target\n",
    "                print(f\"'{col}': 'Other' merged into most frequent category '{final_merge_target}'\")\n",
    "\n",
    " # Save mapping\n",
    "        mappings[col] = {\n",
    "            'group_rare': group_rare,\n",
    "            'final_merge_target': final_merge_target\n",
    "        }\n",
    "\n",
    "    return df, mappings\n",
    "\n",
    "\n",
    "\n",
    "df_rare_cat, cat_mappings = clean_categorical_variables(df_dropped)\n",
    "\n",
    "\n",
    "#remove variables with one category\n",
    "\n",
    "# Step 1: Identify columns dropped from training\n",
    "cols_dropped_one_cat = df_dropped.columns[df_dropped.nunique(dropna=False) <= 1]\n",
    "df_dropped = df_dropped.loc[:, df_dropped.nunique(dropna=False) > 1]\n",
    "\n",
    "# One-hot encode all object or category dtype columns\n",
    "df_dropped = pd.get_dummies(df_dropped, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef2b7e1-a9d4-4a63-8115-4380a351bd40",
   "metadata": {},
   "source": [
    "## 1.9 Chose/train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20dda977-9f23-43cd-8187-c70c392df468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated RMSE scores: [0.11869494 0.10926476 0.14761173 0.12038232 0.11104632]\n",
      "Average RMSE: 0.12140001261117614\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Step 1: Separate features and target\n",
    "X_train = df_dropped.drop(columns=['Id', 'SalePrice_log'])  # Replace 'Price' with your actual target if named differently\n",
    "y = df_dropped['SalePrice_log']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e599199b-7caf-45c4-bfd0-1ac1d9da4010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:2341: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 7.9060432109076855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-4, 4, 50)\n",
    "ridge_cv = RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "ridge_cv.fit(X_train, y)\n",
    "\n",
    "print(\"Best alpha:\", ridge_cv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9d45404-efee-4551-8ea6-2b46d4abf1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated RMSE scores: [0.11649896 0.1063896  0.14470909 0.11129576 0.10533639]\n",
      "Average RMSE: 0.11684595984728216\n"
     ]
    }
   ],
   "source": [
    "#the best\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=7.9060432109076855)\n",
    "\n",
    "model.fit(X_train, y)\n",
    "\n",
    "# Step 4: Cross-validation setup\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 5: Run cross-validation and evaluate using negative RMSE\n",
    "scores = cross_val_score(model, X_train, y, scoring='neg_root_mean_squared_error', cv=cv)\n",
    "\n",
    "# Step 6: Print results\n",
    "print(\"Cross-validated RMSE scores:\", -scores)\n",
    "print(\"Average RMSE:\", -scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3684572-96e7-41a2-9cb2-1c0d2aa4d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5677937501815773, tolerance: 0.017556558139769182\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3294645494453095, tolerance: 0.017556558139769182\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.16898186046393704, tolerance: 0.01721980898283279\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9388887347076453, tolerance: 0.01721980898283279\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5131535747352327, tolerance: 0.01721980898283279\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.37239984776730495, tolerance: 0.01789985587548363\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3006993883585096, tolerance: 0.01789985587548363\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.6607435636367676, tolerance: 0.01789985587548363\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.0006551285568595509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.49628182736332516, tolerance: 0.01760857262184921\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:683: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0801186520303139, tolerance: 0.01760857262184921\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import numpy as np\n",
    "\n",
    "alphas = np.logspace(-4, 4, 50)\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, random_state=0)\n",
    "lasso_cv.fit(X_train, y)\n",
    "\n",
    "print(\"Best alpha:\", lasso_cv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02994725-fd6a-4ddf-8604-0655c15a942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated RMSE scores: [0.11710181 0.10522767 0.14891445 0.10816945 0.1037963 ]\n",
      "Average RMSE: 0.11664193690414784\n"
     ]
    }
   ],
   "source": [
    "#final\n",
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=0.0006551285568595509)\n",
    "\n",
    "model.fit(X_train, y)\n",
    "\n",
    "# Step 4: Cross-validation setup\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 5: Run cross-validation and evaluate using negative RMSE\n",
    "scores = cross_val_score(model, X_train, y, scoring='neg_root_mean_squared_error', cv=cv)\n",
    "\n",
    "# Step 6: Print results\n",
    "print(\"Cross-validated RMSE scores:\", -scores)\n",
    "print(\"Average RMSE:\", -scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee13550-a48d-4321-a3c2-e0178692f49a",
   "metadata": {},
   "source": [
    "### 1.9.1 feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d7a74ad-f593-4d55-80fa-f875e6b08caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Step 1: Separate features and target\n",
    "X_train = df_dropped.drop(columns=['Id', 'SalePrice_log'])  # Replace 'Price' with your actual target if named differently\n",
    "y = df_dropped['SalePrice_log']\n",
    "\n",
    "# 2. Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Feature selection with LassoCV\n",
    "lasso_cv = LassoCV(cv=5, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "# 4. Select features with threshold=\"mean\"\n",
    "selector = SelectFromModel(lasso_cv, threshold=\"mean\", prefit=True)\n",
    "X_train_sel = selector.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62ba6c19-894f-4205-a6cf-01a659dc820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (RidgeCV): 7.9060432109076855\n",
      "Test R² score: 0.7928\n",
      "Test MSE: 0.03\n"
     ]
    }
   ],
   "source": [
    "# 5. RidgeCV model with cross-validation\n",
    "alphas = np.logspace(-4, 4, 50)\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train_sel, y_train)\n",
    "\n",
    "# 6. Evaluate on test set\n",
    "y_pred = ridge_cv.predict(X_train_sel)\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "\n",
    "print(f\"Best alpha (RidgeCV): {ridge_cv.alpha_}\")\n",
    "print(f\"Test R² score: {r2:.4f}\")\n",
    "print(f\"Test MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19a59058-ab51-4b73-8608-98b3801f2ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['MSSubClass', 'OverallQual', 'TotRmsAbvGrd', 'GarageYrBlt', 'GarageArea', 'OpenPorchSF', 'HouseAge', 'YearsSinceRemod', 'BsmtUnfSF_log']\n"
     ]
    }
   ],
   "source": [
    "# 7. View selected features\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(\"Selected features:\", selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79968320-0601-49b9-bc2e-355c9576819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha (RidgeCV): 0.0004498432668969444\n",
      "Test R² score: 0.7928\n",
      "Test MSE: 0.03\n"
     ]
    }
   ],
   "source": [
    "# 5. RidgeCV model with cross-validation\n",
    "alphas = np.logspace(-4, 4, 50)\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5)\n",
    "lasso_cv.fit(X_train_sel, y_train)\n",
    "\n",
    "# 6. Evaluate on test set\n",
    "y_pred = lasso_cv.predict(X_train_sel)\n",
    "r2 = r2_score(y_train, y_pred)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "\n",
    "print(f\"Best alpha (RidgeCV): {lasso_cv.alpha_}\")\n",
    "print(f\"Test R² score: {r2:.4f}\")\n",
    "print(f\"Test MSE: {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd38de-62fa-489b-b52b-fbf7c8850e03",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c81b62d-971a-4219-96bb-39b3c896c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "d_test = pd.read_csv('test.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9f294-5c95-4171-bd8c-95e3f79bd9c6",
   "metadata": {},
   "source": [
    "## 2.1 convert types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65f3f43e-4d44-49a0-8567-624feeb61188",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test[['MSSubClass', 'MoSold', 'YearBuilt', 'YearRemodAdd', 'YrSold']] = d_test[['MSSubClass', 'MoSold', 'YearBuilt', 'YearRemodAdd', 'YrSold']].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a63dcf-3ae1-4d4f-9fb7-95876ca30205",
   "metadata": {},
   "source": [
    "## 2.2 drop variables with most values missing in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "346be79e-7629-4683-8fa6-36b80921ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test.drop(columns=dropped_columns_most_missing, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d06015-95cd-4668-8094-9be78e736a29",
   "metadata": {},
   "source": [
    "## 2.3 Fill missing values with \"Missing\" for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9c17d103-d10a-4e82-9e98-780e25342856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berra\\AppData\\Local\\Temp\\ipykernel_9908\\299113602.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  d_test[cat_cols] = d_test[cat_cols].fillna(\"Missing\")\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "cat_cols = d_test.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Fill missing values with \"Missing\"\n",
    "d_test[cat_cols] = d_test[cat_cols].fillna(\"Missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d134688-30cd-4767-904e-a05c1a12adab",
   "metadata": {},
   "source": [
    "## 2.4 Fill missing values with the mean for quantitative variables from the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27a706c3-037b-4cb4-ae00-f16705f467c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation dans le test set (numeric)\n",
    "d_test.fillna(means, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56454c0e-956e-4a67-9d80-0171982a25f3",
   "metadata": {},
   "source": [
    "## 2.5 create variables like the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a994ebf1-7068-4a67-8998-c179e4b36d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'YearBuilt', 'YearRemodAdd, 'MoSold', 'YrSold' convert to quantitative\n",
    "# List of columns to convert\n",
    "columns_to_convert = ['YearBuilt', 'YearRemodAdd', 'MoSold', 'YrSold']\n",
    "\n",
    "# Convert each to integer safely\n",
    "for col in columns_to_convert:\n",
    "    d_test.loc[:, col] = d_test[col].astype(int)\n",
    "\n",
    "# Set reference year\n",
    "reference_year = d_test['YrSold'].max()\n",
    "\n",
    "# Create quantitative features safely\n",
    "d_test.loc[:, 'HouseAge'] = reference_year - d_test['YearBuilt']\n",
    "d_test.loc[:, 'YearsSinceRemod'] = reference_year - d_test['YearRemodAdd']\n",
    "\n",
    "\n",
    "d_test = d_test.drop([\n",
    "    'YearBuilt', 'YearRemodAdd', 'YrSold', 'MoSold'\n",
    "], axis=1) \n",
    "\n",
    "# Convert object columns to numeric, forcing errors to NaN\n",
    "cols_to_convert = ['HouseAge', 'YearsSinceRemod']\n",
    "d_test[cols_to_convert] = d_test[cols_to_convert].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4fce9-bc48-4575-b79f-172056350c06",
   "metadata": {},
   "source": [
    "## 2.6 Correct skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e3b3acf-f82b-4d54-9989-abebcb51c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#case 1: only binary no log\n",
    "\n",
    "#Most houses don't have low-quality finished square footage.\n",
    "#Very few do, with highly varied amounts.\n",
    "cols_to_binary_only_0 = ['BsmtHalfBath', 'EnclosedPorch', 'ScreenPorch']\n",
    "\n",
    "for col in cols_to_binary_only_0:\n",
    "    d_test[f'Has{col}'] = (d_test[col] > 0).astype(int)\n",
    "    d_test.drop(columns=[col], inplace=True)\n",
    "\n",
    "# = 1 or not\n",
    "\n",
    "d_test['HasKitchen'] = (d_test['KitchenAbvGr'] == 1).astype(int)\n",
    "d_test.drop(columns=['KitchenAbvGr'], inplace=True)\n",
    "\n",
    "\n",
    "#case 2: binary+log\n",
    "\n",
    "cols = ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '2ndFlrSF', 'WoodDeckSF', 'OpenPorchSF']  # Replace with actual column names\n",
    "\n",
    "for col in cols:\n",
    "    d_test[f'Has{col}'] = (d_test[col] > 0).astype(int)\n",
    "    d_test[f'{col}_log'] = np.log1p(d_test[col])\n",
    "\n",
    "#case 3: just drop \n",
    "d_test.drop(columns=['LowQualFinSF','3SsnPorch', 'PoolArea', 'MiscVal'], inplace=True)\n",
    "\n",
    "# case 4: log only\n",
    "\n",
    "# List of variables to transform\n",
    "vars_to_log = ['LotFrontage', 'LotArea', 'TotalBsmtSF', 'GrLivArea',  'BsmtUnfSF', '1stFlrSF']\n",
    "\n",
    "\n",
    "# Create log-transformed versions with \"_log\" suffix\n",
    "for col in vars_to_log:\n",
    "    d_test[col + '_log'] = np.log1p(d_test[col])  # log1p handles zero safely\n",
    "\n",
    "# Drop the original columns\n",
    "d_test.drop(columns=vars_to_log, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75c23de-00b2-40ec-8e87-bdc2a7afae32",
   "metadata": {},
   "source": [
    "## 2.7 Cap outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97e97aab-6472-4839-bf48-0be7899ee412",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = [col for col in num_features if col != \"SalePrice_log\"]\n",
    "\n",
    "for col in num_features:\n",
    "    q_low, q_high = caps[col]\n",
    "    d_test[col] = d_test[col].clip(lower=q_low, upper=q_high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba999a51-26dd-43c9-b6f7-35f5c4b968a7",
   "metadata": {},
   "source": [
    "## 2.8 Categorical variable (merge rare categories into the most frequent in train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e13ae7ca-8efb-46bc-99c5-25eae889d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cat_mapping_to_test(df_test, mappings):\n",
    "    df_test = df_test.copy()\n",
    "\n",
    "    for col, info in mappings.items():\n",
    "        df_test[col] = df_test[col].fillna(\"Missing\")\n",
    "\n",
    "        # Replace rare categories with 'Other'\n",
    "        df_test[col] = df_test[col].apply(lambda x: 'Other' if x in info['group_rare'] else x)\n",
    "\n",
    "        # Merge 'Other' into target if needed\n",
    "        if info['final_merge_target'] is not None:\n",
    "            df_test[col] = df_test[col].replace('Other', info['final_merge_target'])\n",
    "\n",
    "    return df_test\n",
    "# Apply same mappings to test\n",
    "d_test = apply_cat_mapping_to_test(d_test, cat_mappings)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Drop same columns from test set\n",
    "d_test = d_test.drop(columns=cols_dropped_one_cat, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef00e6e-24cc-4f78-a4ae-c7f50d5c24d3",
   "metadata": {},
   "source": [
    "# 1.9 predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ccbca1c0-718c-411e-8075-2d9848189339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode test set\n",
    "X_test = pd.get_dummies(d_test, drop_first=True)\n",
    "X_test = d_test.drop(columns=['Id'], errors='ignore') \n",
    "# Align with training columns (very important!)\n",
    "X_test_aligned = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "\n",
    "y_pred_log = model.predict(X_test_aligned)\n",
    "y_pred = np.expm1(y_pred_log)  # reverse np.log1p()\n",
    "d_test['SalePrice'] = y_pred\n",
    "d_test[['Id', 'SalePrice']].to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85407c12-15b2-4629-9c23-b3411cfb4b6d",
   "metadata": {},
   "source": [
    "### 1.9.1 Prediction with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57a28aac-9bce-4a5f-93eb-f22b550461c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\berra\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode test set\n",
    "X_test = pd.get_dummies(d_test, drop_first=True)\n",
    "X_test = d_test.drop(columns=['Id'], errors='ignore') \n",
    "# Align with training columns (very important!)\n",
    "X_test_aligned = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "X_test_sel = selector.transform(X_test_aligned)\n",
    "\n",
    "y_pred_log = ridge_cv.predict(X_test_sel)\n",
    "y_pred = np.expm1(y_pred_log)  # reverse np.log1p()\n",
    "d_test['SalePrice'] = y_pred\n",
    "d_test[['Id', 'SalePrice']].to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8f4a69e-b312-423e-a1a9-b863214fec56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\berra\\\\House Prices'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
